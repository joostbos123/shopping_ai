{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Set working directory one path up (to root of the repo)\n",
    "os.chdir('..')\n",
    "\n",
    "path_to_dataset = 'datasets/colorful_fashion'\n",
    "\n",
    "CLASS_MAPPING = {\n",
    "    \"0\": \"sunglass\",\n",
    "    \"1\": \"hat\",\n",
    "    \"2\": \"jacket\",\n",
    "    \"3\": \"shirt\",\n",
    "    \"4\": \"pants\",\n",
    "    \"5\": \"shorts\",\n",
    "    \"6\": \"skirt\",\n",
    "    \"7\": \"dress\",\n",
    "    \"8\": \"bag\",\n",
    "    \"9\": \"shoe\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or empty folders to store the yolo dataset\n",
    "for set_type in ['train', 'val']:\n",
    "    for file_type in ['images', 'labels']:\n",
    "        # Create directory when it does not exist\n",
    "        if not os.path.exists(f'{path_to_dataset}/yolo/{set_type}/{file_type}'):\n",
    "            os.makedirs(f'{path_to_dataset}/yolo/{set_type}/{file_type}')\n",
    "        # Empty folder if it already exists\n",
    "        else:\n",
    "            for file in os.scandir(f'{path_to_dataset}/yolo/{set_type}/{file_type}'):\n",
    "                os.remove(file.path)\n",
    "\n",
    "# Create train set\n",
    "with open(f'{path_to_dataset}/ImageSets/Main/trainval.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        name = line.strip()  # removes the new line character\n",
    "\n",
    "        # Copy image to train set\n",
    "        shutil.copyfile(f'{path_to_dataset}/JPEGImages/{name}.jpg', f'../datasets/colorful_fashion/yolo/train/images/{name}.jpg')    \n",
    "\n",
    "        # Copy label to train set\n",
    "        shutil.copyfile(f'{path_to_dataset}/Annotations_txt/{name}.txt', f'../datasets/colorful_fashion/yolo/train/labels/{name}.txt')\n",
    "\n",
    "# Create val set\n",
    "with open(f'{path_to_dataset}/ImageSets/Main/test.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        name = line.strip()  # removes the new line character\n",
    "\n",
    "        # Copy image to val set\n",
    "        shutil.copyfile(f'{path_to_dataset}/JPEGImages/{name}.jpg', f'../datasets/colorful_fashion/yolo/val/images/{name}.jpg')    \n",
    "\n",
    "        # Copy label to val set\n",
    "        shutil.copyfile(f'{path_to_dataset}/Annotations_txt/{name}.txt', f'../datasets/colorful_fashion/yolo/val/labels/{name}.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_exploration import get_image_labels_local, print_dataset_statistics, plot_yolo_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels_train = get_image_labels_local(yolo_folder=f'{path_to_dataset}/yolo', dataset_type='train')\n",
    "image_labels_val = get_image_labels_local(yolo_folder=f'{path_to_dataset}/yolo', dataset_type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set consists of 2145 images\n",
      "The labels are distributed as follows:\n",
      " shoe        2081\n",
      "shirt       1486\n",
      "bag         1188\n",
      "jacket       777\n",
      "skirt        697\n",
      "dress        550\n",
      "pants        508\n",
      "shorts       395\n",
      "sunglass     343\n",
      "hat          273\n",
      "Name: count, dtype: int64\n",
      "Val set consists of 537 images\n",
      "The labels are distributed as follows:\n",
      " shoe        520\n",
      "shirt       366\n",
      "bag         274\n",
      "skirt       186\n",
      "jacket      181\n",
      "dress       128\n",
      "pants       114\n",
      "shorts      107\n",
      "sunglass     82\n",
      "hat          77\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_dataset_statistics(image_labels_train, 'train', CLASS_MAPPING)\n",
    "print_dataset_statistics(image_labels_val, 'val', CLASS_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_yolo_labels(yolo_folder=f'{path_to_dataset}/yolo', yolo_plotting_folder=f'{path_to_dataset}/yolo_plotting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define variables for data config yaml\n",
    "path = '../../input/data'\n",
    "images_path_train = 'train/images'\n",
    "images_path_val = 'val/images'\n",
    "\n",
    "class_count = len(CLASS_MAPPING)\n",
    "class_names = list(CLASS_MAPPING.values())\n",
    "\n",
    "data = {\n",
    "    'path': path,\n",
    "    'train': images_path_train,\n",
    "    'val': images_path_val,\n",
    "    'nc': class_count,\n",
    "    'names': class_names,\n",
    "}\n",
    "\n",
    "path_to_yml = 'yolov8/data.yaml'\n",
    "\n",
    "\n",
    "# Create/update yaml containing the data config\n",
    "with open(path_to_yml, 'w') as outfile:\n",
    "    yaml.dump(data, outfile, sort_keys=False, default_flow_style=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "# Job name\n",
    "now = datetime.now()\n",
    "job_name = f'yolov8-{JOB_NAME}-{now.strftime(\"%Y-%m-%d-%H-%M-%S\")}'\n",
    "\n",
    "# Paths to datasets\n",
    "s3_path_dataset_train = f's3://{S3_BUCKET_INPUT}/{S3_PREFIX_YOLO_DATASET}/train/'\n",
    "s3_path_dataset_val = f's3://{S3_BUCKET_INPUT}/{S3_PREFIX_YOLO_DATASET}/val/'\n",
    "\n",
    "# Metrics for training\n",
    "metric_definitions = [{'Name': 'Epoch_best', 'Regex': 'Epoch_best=(.*?);'},\n",
    "                      {'Name': 'Precision', 'Regex': 'Precision=(.*?);'},\n",
    "                      {'Name': 'Recall', 'Regex': 'Recall=(.*?);'},\n",
    "                      {'Name': 'mAP@.5', 'Regex': 'mAP@.5=(.*?);'},\n",
    "                      {'Name': 'mAP@.5:.95', 'Regex': 'mAP@.5:.95=(.*?);'}]\n",
    "\n",
    "# Datasets used during training \n",
    "inputs = {'train': s3_path_dataset_train, 'val': s3_path_dataset_val}\n",
    "\n",
    "# Define training settings\n",
    "pytorch_estimator = PyTorch(\n",
    "                    instance_type='ml.p3.2xlarge', # ml.p3.2xlarge or ml.p3.8xlarge (gpu) or ml.c5.9xlarge (cpu)\n",
    "                    instance_count=1,\n",
    "                    entry_point=\"train.py\",\n",
    "                    source_dir=\"yolov8\",\n",
    "                    role = 'arn:aws:iam::020223827588:role/model-training-cv-playground-sagemaker-execution', #get_execution_role(),\n",
    "                    job_name = job_name,\n",
    "                    checkpoint_s3_uri = f's3://model-training-cv-playground-model-artifact/YOLOV8/{job_name}/checkpoints',\n",
    "                    use_spot_instances = True,\n",
    "                    max_wait = 14800, # 4hours\n",
    "                    max_run = 7200, # 2hours\n",
    "                    image_uri = '020223827588.dkr.ecr.eu-west-1.amazonaws.com/ultralytics-yolov8-sagemaker:latest',\n",
    "                    hyperparameters = {'training-job-name': job_name,\n",
    "                                       # For all YOLOV8 default parameters, check https://github.com/ultralytics/ultralytics/blob/790f9c067c9a3548acf7c6925cea14de9ad77787/ultralytics/yolo/cfg/default.yaml\n",
    "                                       # Train settings --------------------------------------------------------------\n",
    "                                       'base_model': 'yolov8l.pt', # path to model file, i.e. yolov8n.pt, yolov8n.yaml\n",
    "                                       'data': 'data.yaml', # path to data file, i.e. coco128.yaml\n",
    "                                       'epochs': 20, # number of epochs to train for\n",
    "                                       'patience': 20, # epochs to wait for no observable improvement for early stopping of training\n",
    "                                       'batch': -1, # number of images per batch (-1 for AutoBatch)\n",
    "                                       'imgsz': 1280, # size of input images as integer or w,h\n",
    "                                       'save_period': 10, # Save checkpoint every x epochs (disabled if < 1)\n",
    "                                       'image_weights': False, # use weighted image selection for training\n",
    "                                       'rect': False, # support rectangular training if mode='train', support rectangular evaluation if mode='val'\n",
    "                                       'close_mosaic': 10, # disable mosaic augmentation for final 10 epochs\n",
    "                                       'lr0': 0.01, # initial learning rate (i.e. SGD=1E-2, Adam=1E-3)\n",
    "                                       'lrf': 0.01, # final learning rate (lr0 * lrf)\n",
    "                                       'dropout': 0.0,\n",
    "                                       # Hyper parameters for data augmentation --------------------------------------\n",
    "                                       'degrees': 0.0, # image rotation (+/- deg)\n",
    "                                       'translate': 0.1,  # image translation (+/- fraction)\n",
    "                                       'scale': 0.5,  # image scale (+/- gain)\n",
    "                                       'shear': 0.0,  # image shear (+/- deg)\n",
    "                                       'perspective': 0.0,  # image perspective (+/- fraction), range 0-0.001\n",
    "                                       'flipud': 0.0,  # image flip up-down (probability)\n",
    "                                       'fliplr': 0.5,  # image flip left-right (probability)\n",
    "                                       'mosaic': 1.0,  # image mosaic (probability)\n",
    "                                       'mixup': 0.0,  # image mixup (probability)\n",
    "                                       'copy_paste': 0.1  # segment copy-paste (probability)\n",
    "                                      },\n",
    "                    metric_definitions=metric_definitions)\n",
    "\n",
    "# Start a SageMaker training job\n",
    "pytorch_estimator.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.tuner import (\n",
    "#     IntegerParameter,\n",
    "#     CategoricalParameter,\n",
    "#     ContinuousParameter,\n",
    "#     HyperparameterTuner,\n",
    "# )\n",
    "\n",
    "# hyperparameter_ranges = {\n",
    "#     \"lr0\": ContinuousParameter(0.0001, 0.01),\n",
    "#     \"dropout\": ContinuousParameter(0.0, 0.4)\n",
    "#     }\n",
    "\n",
    "\n",
    "# tuner = HyperparameterTuner(\n",
    "#     estimator =pytorch_estimator,\n",
    "#     objective_metric_name='mAP@.5:.95',\n",
    "#     hyperparameter_ranges=hyperparameter_ranges,\n",
    "#     metric_definitions=metric_definitions,\n",
    "#     strategy='Bayesian',\n",
    "#     objective_type='Maximize',\n",
    "#     max_jobs=6,\n",
    "#     max_parallel_jobs=1)\n",
    "\n",
    "# tuner.fit(inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Download model from S3\n",
    "s3_client.download_file('model-training-cv-playground-model-artifact', 'YOLOV8/yolov8-soccer-player-detection-2023-05-03-16-44-18/checkpoints/weights/best.pt', 'best_soccer_player.pt')\n",
    "\n",
    "# Download test image from S3\n",
    "s3_client.download_file('model-training-cv-playground-model-input', 'datasets/soccer_player/test/images/1-fps-2_00001_jpeg_jpg.rf.03aa7dfbdbc3d0a5482ea68a7f9a8a8d.jpg', '1-fps-2_00001_jpeg_jpg.rf.03aa7dfbdbc3d0a5482ea68a7f9a8a8d.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('best_soccer_player.pt')\n",
    "\n",
    "# Perform inference on test image\n",
    "results = model('1-fps-2_00001_jpeg_jpg.rf.03aa7dfbdbc3d0a5482ea68a7f9a8a8d.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics.yolo.utils.plotting import Annotator\n",
    "\n",
    "img = cv2.imread('1-fps-2_00001_jpeg_jpg.rf.03aa7dfbdbc3d0a5482ea68a7f9a8a8d.jpg', cv2.COLOR_BGR2RGB)\n",
    "\n",
    "annotator = Annotator(img)\n",
    "\n",
    "for box in results[0].boxes:\n",
    "    \n",
    "    if float(box.conf) >0.5:\n",
    "        b = box.xyxy[0]  # get box coordinates in (top, left, bottom, right) format\n",
    "        c = box.cls\n",
    "        annotator.box_label(b, model.names[int(c)])\n",
    "        \n",
    "cv2.imwrite('output.png', img)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopping_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
